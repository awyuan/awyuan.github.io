---
---

@INPROCEEDINGS {,
author = {A. Yuan and A. Oprea and C. Tan},
booktitle = {2024 IEEE Symposium on Security and Privacy (SP)},
title = {Dropout Attacks},
year = {2024},
volume = {},
issn = {2375-1207},
pages = {26-26},
abstract = {Dropout is a common operator in deep learning, aiming to prevent overfitting by randomly dropping neurons during training. This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK. DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random. We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios. These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class. In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (81.7% â†’ 47.1%) without incurring any degradation in model accuracy},
keywords = {ml security;ml attacks},
doi = {10.1109/SP54263.2024.00026},
url = {https://doi.ieeecomputersociety.org/10.1109/SP54263.2024.00026},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@inproceedings{pal-etal-2023-future,
    title = "Future Lens: Anticipating Subsequent Tokens from a Single Hidden State",
    author = "Pal, Koyena  and
      Sun, Jiuding  and
      Yuan, Andrew  and
      Wallace, Byron  and
      Bau, David",
    editor = "Jiang, Jing  and
      Reitter, David  and
      Deng, Shumin",
    booktitle = "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-1.37",
    doi = "10.18653/v1/2023.conll-1.37",
    pages = "548--560",
    abstract = "We conjecture that hidden state vectors corresponding to individual input tokens encode information sufficient to accurately predict several tokens ahead. More concretely, in this paper we ask: Given a hidden (internal) representation of a single token at position t in an input, can we reliably anticipate the tokens that will appear at positions {\mbox{$\geq$}} t + 2? To test this, we measure linear approximation and causal intervention methods in GPT-J-6B to evaluate the degree to which individual hidden states in the network contain signal rich enough to predict future hidden states and, ultimately, token outputs. We find that, at some layers, we can approximate a model{'}s output with more than 48{\%} accuracy with respect to its prediction of subsequent tokens through a single hidden state. Finally we present a {``}Future Lens{''} visualization that uses these methods to create a new view of transformer states.",
}